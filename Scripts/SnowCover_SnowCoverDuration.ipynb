{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27de6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to clear all cloud- and no-data flagged pixels for each hydrological year\n",
    "# and interpolate these data/ cloud gaps  to identify the snow cover (SC) status on the ground\n",
    "# and to calculate the snow cover duration (SCD) for the current hydrological year\n",
    "# and each pixel within the study region (Central Asia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running this script: \n",
    "# 1. Use the script Download_MODIS.ipynb to download the needed MODIS Snow Cover Terra and Aqua data\n",
    "# 2. Use the script ReplaceMissing_MODIS.ipynb to search for missing MODIS Terra files \n",
    "# and replace them with the corresponding MODIS Aqua files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9be9b7",
   "metadata": {},
   "source": [
    "After running the two previous scripts there should be a terra and an aqua file folder with the all the scenes from September 2012 to September 2022. The terra file folder needs to be manually split into multiple subfolders, one for each hydrological year, to reduce to computational power needed. They should be named clearly: e.g. MOD10A1_12-13, MOD10A1_13-14, ..., MOD10A1_21-22\n",
    "Additionally, folders for storing the GeoTiff files of the snow cover need to be set up. They also should be named clearly: e.g. SC_12-13, SC_13-14, ..., SC_21-22.\n",
    "Lastly, one folder for saving all the SCD rasters for the ten hydrological years needs to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the needed modules\n",
    "import os\n",
    "import shutil\n",
    "import xarray\n",
    "import h5py\n",
    "import glob\n",
    "import sys\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define needed folders, files and variables\n",
    "\n",
    "# folders\n",
    "# MODIS terra folder: e.g. MOD10A1_20-21 \n",
    "MODIS_terra_file_folder = 'path/to/your/MODIS_Terra_folder/'\n",
    "MODIS_aqua_file_folder = 'path/to/your/MODIS_Aqua_folder/'\n",
    "# Snow cover folder: e.g. SC_20-21\n",
    "SC_output_folder = 'path/to/your/SC_output_folder/'\n",
    "SCD_output_folder = 'path/to/your/SCD_output_folder/'\n",
    "\n",
    "# files\n",
    "SRTM = 'path/to/h23v04_srtm_subset_corrected.tif'\n",
    "SCD_output_array = 'SCD_current_hydrological_year.npz'\n",
    "SCD_output_raster = 'SCD_current_hydrological_year.tif'\n",
    "\n",
    "# variables\n",
    "cloud_threshold = 10\n",
    "NDSI_threshold_not_so_certain_snow = 10\n",
    "NDSI_threshold_quite_certain_snow = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "# read in a MODIS .hdf files\n",
    "def read_MODIS_snow(file):\n",
    "    hdf_ds = gdal.Open(file, gdal.GA_ReadOnly)\n",
    "    band_ds = gdal.Open(hdf_ds.GetSubDatasets()[0][0], gdal.GA_ReadOnly) # 'Snow_Cover_Daily_Tile' v5, 'NDSI_Snow_Cover' v6\n",
    "    data = band_ds.ReadAsArray()\n",
    "    temp = np.copy(data)\n",
    "    data = np.full(temp.shape, 0, dtype=np.uint8) # Missing Data (cloud, polar night etc.)\n",
    "    data = np.where(temp < NDSI_threshold_not_so_certain_snow, 32, data) # Snow Free (NDSI < 0.1)\n",
    "    data = np.where((temp >= NDSI_threshold_not_so_certain_snow) & (temp < NDSI_threshold_quite_certain_snow), 64, data) # Snow not so certain  (NDSI >= 0.1 & NDSI < 0.4)\n",
    "    data = np.where((temp >= NDSI_threshold_quite_certain_snow) & (temp <= 100), 128, data) # Snow (NDSI >= 0.4)\n",
    "    data = np.where(temp == 237, 16, data) # Inland Water\n",
    "    data = np.where(temp == 239, 8, data) # Ocean\n",
    "    snow_qa_ds = gdal.Open(hdf_ds.GetSubDatasets()[2][0], gdal.GA_ReadOnly) # \"NDSI_Snow_Cover_Algorithm_Flags_QA\"\n",
    "    snow_qa = snow_qa_ds.ReadAsArray()\n",
    "    snow_qa = snow_qa[:,:,np.newaxis]\n",
    "    snow_qa_bits = np.unpackbits(snow_qa, axis=-1, bitorder='little') # Bit 0: Inland water\n",
    "    data = np.where((data > 16) & (snow_qa_bits[:,:,0]==1), 16, data) #Pixels are re-assigned to water\n",
    "    return data\n",
    "\n",
    "# read in a geotiff file using GDAL\n",
    "def read_tiff(file):\n",
    "    ds = gdal.Open(file, gdal.GA_ReadOnly)\n",
    "    band = ds.GetRasterBand(1)\n",
    "    arr = band.ReadAsArray()\n",
    "    return arr\n",
    "\n",
    "# write geotiffs using GDAL (commented out the compresion because compressed files can not be read in later)\n",
    "def write_tiff(outfile, data, proj_info, dtype=gdal.GDT_Byte):\n",
    "    x,y = data.shape\n",
    "    dst = gdal.GetDriverByName('GTiff').Create(outfile,y,x,1,dtype)#options=['COMPRESS=DEFLATE']\n",
    "    dst.SetGeoTransform(proj_info[0])\n",
    "    dst.SetProjection(proj_info[1])\n",
    "    dst.GetRasterBand(1).WriteArray(data)\n",
    "    dst = None\n",
    "    return ''\n",
    "\n",
    "# write 16 bit geotiffs using GDAL (commented out the compresion because compressed files can not be read in later)\n",
    "def write_tiff(outfile, data, proj_info, dtype=gdal.GDT_UInt16):\n",
    "    x,y = data.shape\n",
    "    dst = gdal.GetDriverByName('GTiff').Create(outfile,y,x,1,dtype)#options=['COMPRESS=DEFLATE']\n",
    "    dst.SetGeoTransform(proj_info[0])\n",
    "    dst.SetProjection(proj_info[1])\n",
    "    dst.GetRasterBand(1).WriteArray(data)\n",
    "    dst = None\n",
    "    return ''\n",
    "\n",
    "# 3-day temporal interpolation\n",
    "def interpolate_3day(GSP_data_stack, missing_value=0):\n",
    "    temporary_GSP_stack = np.copy(GSP_data_stack)\n",
    "    N = GSP_data_stack.shape[2]\n",
    "    for j in range(N-1):\n",
    "        if j>0:\n",
    "            actual_data = np.copy(temporary_GSP_stack[:,:,j]) # today's data\n",
    "            prior_data = np.copy(temporary_GSP_stack[:,:,j-1])\n",
    "            next_data = np.copy(temporary_GSP_stack[:,:,j+1])\n",
    "            actual_data = np.where(actual_data == missing_value, next_data, actual_data)\n",
    "            actual_data = np.where(actual_data == missing_value, prior_data, actual_data)\n",
    "            changed_values = np.where((temporary_GSP_stack[:,:,j] == missing_value) & (actual_data != missing_value))\n",
    "            actual_data[changed_values]+=1\n",
    "            GSP_data_stack[:,:,j] = actual_data[:,:]\n",
    "    temporary_GSP_stack = None\n",
    "    return GSP_data_stack\n",
    "\n",
    "# SRTM snowline interpolation\n",
    "def srtm_interpolation(GSP_data_stack, dem, cloud_threshold):\n",
    "    N = GSP_data_stack.shape[2]\n",
    "    for j in range(N):\n",
    "        all_cloud_pixels=np.count_nonzero(GSP_data_stack[:,:,j] == 0)\n",
    "        cloud_stats=all_cloud_pixels/GSP_data_stack[:,:,j].size * 100\n",
    "        if cloud_stats<=cloud_threshold:\n",
    "            #print(\"Entered if with {}\".format(cloud_stats))\n",
    "            print('Cloud cover below ' + str(cloud_threshold)+ ' in scene '+ str(j))\n",
    "            free = np.where((GSP_data_stack[:,:,j] >= 32) & (GSP_data_stack[:,:,j] < 64))\n",
    "            snow = np.where(GSP_data_stack[:,:,j] >= 64)\n",
    "            fullsnow = np.where(GSP_data_stack[:,:,j] >= 128)\n",
    "            step3_data_subset = np.copy(GSP_data_stack[:,:,j])\n",
    "            if len(free[0])!=0:\n",
    "                snow_free_snowline = np.nanmax(dem[free])\n",
    "            else:\n",
    "                snow_free_snowline = 0.0\n",
    "            if len(snow[0])!=0:\n",
    "                snow_covered_snowline = np.nanmin(dem[snow])\n",
    "                if len(fullsnow[0])!=0:\n",
    "                    snow_fullcovered_snowline = np.nanmax(dem[snow])\n",
    "                else:\n",
    "                    snow_fullcovered_snowline = np.nanmax(dem)\n",
    "            else:\n",
    "                snow_covered_snowline = np.nanmax(dem)\n",
    "                snow_fullcovered_snowline = np.nanmax(dem)\n",
    "            #find the pixels to recode:\n",
    "            step3_data_subset = np.where((step3_data_subset == 0) & (dem > snow_free_snowline), 66, step3_data_subset) # cloud to snow\n",
    "            step3_data_subset = np.where((step3_data_subset == 0) & (dem < snow_covered_snowline), 34, step3_data_subset) # cloud to free\n",
    "            step3_data_subset = np.where((step3_data_subset == 66) & (dem > snow_fullcovered_snowline), 130, step3_data_subset)\n",
    "            GSP_data_stack[:,:,j] = step3_data_subset[:,:] \n",
    "    return GSP_data_stack\n",
    "\n",
    "# seasonal interpolation\n",
    "def seasonal_interpolation(GSP_data_stack, proj_info):\n",
    "    N = GSP_data_stack.shape[2]\n",
    "    last_valid = np.full([2400,2400],1,dtype=np.uint8)\n",
    "    days_to_cloudfree = np.full([2400,2400],0,dtype=np.uint16)\n",
    "    for j in range(N):\n",
    "        print(j)\n",
    "        os.makedirs(SC_output_folder, exist_ok=True)\n",
    "        season_name = 'SEASON10A1.A'+str(j+1)+'.tif'\n",
    "        if j < 100:\n",
    "            season_name = 'SEASON10A1.A0'+str(j+1)+'.tif'\n",
    "        if j < 10:\n",
    "            season_name = 'SEASON10A1.A00'+str(j+1)+'.tif'\n",
    "        #accuracy_name = 'ACC10A1.A%04i%03i.%s.tif'%(timestamp.tm_year, timestamp.tm_yday, xdct['tile'])\n",
    "        #cloud_distance_name = 'XCC10A1.A%04i%03i.%s.tif'%(timestamp.tm_year, timestamp.tm_yday, xdct['tile'])\n",
    "        tmp_gsp = np.copy(GSP_data_stack[:,:,j])\n",
    "        tmp_gsp = np.where((tmp_gsp==0)&(last_valid!=0),last_valid,tmp_gsp)\n",
    "        seasonal_bit = np.unpackbits(tmp_gsp[:,:,np.newaxis], axis=-1, bitorder='little')[:,:,2] # check if seasonal bit is already set\n",
    "        days_to_cloudfree = np.where((GSP_data_stack[:,:,j]!=0),0,days_to_cloudfree+1)\n",
    "        last_valid = np.where((GSP_data_stack[:,:,j]!=0),GSP_data_stack[:,:,j],last_valid)\n",
    "        tmp_gsp = np.where((days_to_cloudfree!=0)&(tmp_gsp>=8)&(seasonal_bit==0),tmp_gsp+4,tmp_gsp)\n",
    "        #tmp_gsp = np.where((tmp_gsp != 0)&(land_mask == 0),0,tmp_gsp) # mask no data\n",
    "        GSP_data_stack[:,:,j] = tmp_gsp\n",
    "        write_tiff(os.path.join(SC_output_folder, season_name), tmp_gsp, proj_info)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab987739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists (MODIS Terra and Aqua) and fill them with the respective file names\n",
    "# for this analysis we only need the terra_files\n",
    "\n",
    "# create two empty lists\n",
    "terra_files, aqua_files = [], []\n",
    "\n",
    "for filename in os.listdir(MODIS_terra_file_folder)+os.listdir(MODIS_aqua_file_folder):\n",
    "            #add_log_entry(log_file,'File found %s'%filename)\n",
    "            version = filename.split('.')[3]\n",
    "            if version == '006' or version == '061':\n",
    "                if filename.startswith('MOD10A1.') & filename.endswith('hdf'):\n",
    "                    terra_file = os.path.join(MODIS_terra_file_folder, filename)\n",
    "                    #add_log_entry(log_file,'Terra File found %s'%terra_file)\n",
    "                    terra_files.append(terra_file)\n",
    "                if filename.startswith('MYD10A1.') & filename.endswith('hdf'):\n",
    "                    aqua_file = os.path.join(MODIS_aqua_file_folder, filename)\n",
    "                    #add_log_entry(log_file,'Aqua File found %s'%aqua_file)\n",
    "                    aqua_files.append(aqua_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23306635",
   "metadata": {},
   "outputs": [],
   "source": [
    "terra_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the files from the terra_files list and store the information in an array (GSP_data_stack)\n",
    "N = len(terra_files)\n",
    "GSP_data_stack = np.full([2400, 2400, N], 0, dtype = np.uint8)\n",
    "\n",
    "for i in range(len(terra_files)):\n",
    "    print(terra_files[i])\n",
    "    terra_data = read_MODIS_snow(terra_files[i])\n",
    "    GSP_data_stack[:,:,i] = terra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSP_data_stack[:,:,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fecb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Plot the cloud cover percentage for each day as well as the min, max, and mean of one examplary year \n",
    "\n",
    "# get the cloud cover percentage for every scene of the year\n",
    "N = GSP_data_stack.shape[2]\n",
    "cloud_stats_list = []\n",
    "\n",
    "for j in range(N):\n",
    "    all_cloud_pixels = np.count_nonzero(GSP_data_stack[:,:,j] == 0)\n",
    "    cloud_stats = all_cloud_pixels / GSP_data_stack[:,:,j].size*100\n",
    "    cloud_stats_list.append(cloud_stats)\n",
    "\n",
    "# create x-axis values corresponding to the day of the hydrological year\n",
    "x_values = range(len(cloud_stats_list))\n",
    "\n",
    "# plot the list with the cloud cover statistics as a line plot\n",
    "plt.plot(x_values, cloud_stats_list, marker = 'o', markersize = 3, linestyle = '-')\n",
    "\n",
    "# calculate the min, max, and mean percentage\n",
    "mean_percentage = sum(cloud_stats_list) / len(cloud_stats_list)\n",
    "min_value = min(cloud_stats_list)\n",
    "max_value = max(cloud_stats_list)\n",
    "\n",
    "# plot the min, max, and mean percentage as a horizontal line\n",
    "plt.axhline(y = mean_percentage, color = 'r', linestyle = '--', label = f'Mean Percentage: {mean_percentage:.2f}')\n",
    "plt.axhline(y = min_value, color = 'g', linestyle = '--', label = f'Minimum Value: {min_value:.2f}')\n",
    "plt.axhline(y = max_value, color = 'b', linestyle = '--', label = f'Maximum Value: {max_value:.2f}')\n",
    "\n",
    "# Add a horizontal line at y = 0.5\n",
    "y_horizontal = 10\n",
    "plt.axhline(y_horizontal, color='yellow', linestyle='--', label='Horizontal Line at y=0.5')\n",
    "\n",
    "\n",
    "# add labels, title, and legend\n",
    "plt.xlabel('Day of the Year')\n",
    "plt.ylabel('Cloud Cover [%]')\n",
    "plt.title('Cloud Cover')\n",
    "plt.legend(loc = 'center left', bbox_to_anchor = (1.0, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-day temporal interpolation\n",
    "GSP_data_stack = interpolate_3day(GSP_data_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794af753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# srtm snowline elevation interpolation\n",
    "dem = read_tiff(SRTM)\n",
    "GSP_data_stack = srtm_interpolation(GSP_data_stack, dem, cloud_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c12efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal interpolation\n",
    "\n",
    "# get the projection information to be able to write out the result\n",
    "hdf_ds = gdal.Open(terra_files[1])\n",
    "ds = gdal.Open(hdf_ds.GetSubDatasets()[0][0])\n",
    "proj_info = ds.GetGeoTransform(), ds.GetProjection()\n",
    "\n",
    "# full seasonal filter\n",
    "done = seasonal_interpolation(GSP_data_stack, proj_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_ds = gdal.Open(terra_files[1])\n",
    "ds = gdal.Open(hdf_ds.GetSubDatasets()[0][0])\n",
    "proj_info = ds.GetGeoTransform(), ds.GetProjection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack snow cover ouput rasters to an array\n",
    "\n",
    "# list of SC output files\n",
    "SC_output_files = []\n",
    "\n",
    "for filename in os.listdir(SC_output_folder):\n",
    "    if filename.endswith('tif'):\n",
    "        SC_output_file = os.path.join(SC_output_folder, filename)\n",
    "        SC_output_files.append(SC_output_file)\n",
    "\n",
    "# open the SC raster files and stack them in an array\n",
    "N = len(SC_output_files)\n",
    "raster_data_list = []\n",
    "\n",
    "for i in range(N):\n",
    "    # open the .tif SC file\n",
    "    with rasterio.open(SC_output_files[i]) as src:\n",
    "        # read the raster data as  array\n",
    "        raster_data_list.append(src.read(1))\n",
    "        \n",
    "raster_data_array = np.stack(raster_data_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the SCD\n",
    "\n",
    "# copy the array (to keep the unchanged original)\n",
    "rda = np.copy(raster_data_array)\n",
    "\n",
    "# binary snow/ no snow mask\n",
    "# set all snow pixels (pixels > 128) to 1 and all no snow pixels (pixels < 128) to 0\n",
    "rda[rda < 128] = 0\n",
    "rda[rda >= 128] = 1\n",
    "\n",
    "# count the number of days with snow cover (1) per pixel\n",
    "rda_sum = np.sum(rda, axis = 0)\n",
    "\n",
    "# plot showing the snow cover duration for the hydrological year\n",
    "plt.imshow(rda_sum)\n",
    "plt.title('Snow Cover Duration one Hydrological Year')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the array containing the SCD\n",
    "np.save(os.path.join(SCD_output_folder, SCD_output_array), rda_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43bf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the SCD array as .tif file\n",
    "write_tiff(os.path.join(SCD_output_folder, SCD_output_raster), rda_sum, proj_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb699f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rda_sum.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
